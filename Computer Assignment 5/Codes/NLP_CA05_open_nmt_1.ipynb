{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y39sQj97QGrT",
        "outputId": "09ac54f4-f99b-419f-b577-b6375fd7474b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.4-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 7.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 37.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 43.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting libwapiti\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 8.0 MB/s \n",
            "\u001b[?25hCollecting parsivar\n",
            "  Downloading parsivar-0.2.3.tar.gz (36.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 36.2 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from libwapiti) (1.15.0)\n",
            "Collecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 41.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: libwapiti, parsivar, nltk\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154825 sha256=de6c8ea14e52b36738daa849a32de17a31a2bb78dd0c5e6a22559f9264f1ab1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "  Building wheel for parsivar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsivar: filename=parsivar-0.2.3-py3-none-any.whl size=36492972 sha256=d3800448713bb6cbecad569f96ea4d59b01da5cab3864b4950ff9e4fcdc1ab79\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/67/7a/49cbf08f64d3f76a26eceaf0e481a40e233f05d4356875cbed\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449920 sha256=d960595c8e40725fc6c715850779fed746ff59cc2f19d535439533e1955d32c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "Successfully built libwapiti parsivar nltk\n",
            "Installing collected packages: nltk, parsivar, libwapiti\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed libwapiti-0.2.1 nltk-3.4.5 parsivar-0.2.3\n",
            "Name: libwapiti\n",
            "Version: 0.2.1\n",
            "Summary: Python bindings for libwapiti\n",
            "Home-page: UNKNOWN\n",
            "Author: Adam Svanberg\n",
            "Author-email: asvanberg@gmail.com\n",
            "License: UNKNOWN\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: six\n",
            "Required-by: \n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220527125636)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.46.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 8.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install libwapiti parsivar\n",
        "!pip show libwapiti\n",
        "!pip install tensorflow\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "je_hjFVqPjEi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import MarianMTModel, MarianConfig\n",
        "from transformers import MarianTokenizer\n",
        "from parsivar import *\n",
        "import pprint\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pxoen62Y9gA",
        "outputId": "ffc95ff8-7e2a-45e2-fb39-7005195c5bac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lN8ICewOrOEl"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(BPE())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZF2z4MgMqee8",
        "outputId": "b25ad125-8c89-48cc-e1a8-cd3377b88804"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/data.zip\n",
            "   creating: data/\n",
            "  inflating: data/test.en            \n",
            "  inflating: data/test.fa            \n",
            "  inflating: data/train.en           \n",
            "  inflating: data/train.fa           \n",
            "  inflating: data/valid.en           \n",
            "  inflating: data/valid.fa           \n"
          ]
        }
      ],
      "source": [
        "#آوردن دیتا از درایو\n",
        "!cp /content/drive/MyDrive/NLP/CA05/data.zip /content/data.zip\n",
        "!unzip /content/data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PHNTS9-4VxS",
        "outputId": "dfe133cf-b277-418c-e15b-8d1e9e9ce68f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  spell.zip\n",
            "  inflating: mybigram_lm.pckl        \n",
            "  inflating: onegram.pckl            \n"
          ]
        }
      ],
      "source": [
        "!cp /content/drive/MyDrive/NLP/CA05/spell.zip /content/spell.zip\n",
        "!unzip spell.zip\n",
        "!mkdir /usr/local/lib/python3.7/dist-packages/parsivar/resource/spell\n",
        "!mv /content/mybigram_lm.pckl /usr/local/lib/python3.7/dist-packages/parsivar/resource/spell\n",
        "!mv /content/onegram.pckl /usr/local/lib/python3.7/dist-packages/parsivar/resource/spell\n",
        "#کپی کردن فایل های مورد نیاز برای تصحیح متن فارسی"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT7T4aEE-09b",
        "outputId": "691c4d8f-c89c-42ec-b4b2-dbc9dd2d0684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[K     |████████████████████████████████| 622 kB 7.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622382 sha256=d8e8400db9a49eeaa1cf6b5b6e9e7c9a5a87873c5819250d6fe240f6367589d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/d4/37/8244101ad50b0f7d9bffd93ce58ed7991ee1753b290923934b\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install autocorrect #کتاب خانه تصحیح متن انگلیسی"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZoG4DQvK4NlH"
      },
      "outputs": [],
      "source": [
        "from parsivar import SpellCheck\n",
        "from autocorrect import Speller\n",
        "spell = Speller(lang='en')\n",
        "spcheck = SpellCheck()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Xf-WiBV1_2ZE"
      },
      "outputs": [],
      "source": [
        "def perNormalizer(sentence):\n",
        "    sentence = sentence.replace('ي', 'ی')\n",
        "    sentence = sentence.replace('ك', 'ک')\n",
        "    sentence = sentence.replace('ٔ', '')\n",
        "    sentence = spcheck.spell_corrector(sentence)\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cKwMk_8Y_L1b",
        "outputId": "f69982c5-87b9-49f7-ac60-93119a068626"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'car'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spell('caaar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGKXpj8B4Nev"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EatPV9PG4Nb4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzB5uBV34NXq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LptlmloJ4NUD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw8jZDheDVIm"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0acYboJaAYgv"
      },
      "outputs": [],
      "source": [
        "#تابع پیش پردازش فایل ها\n",
        "def load_and_pre_processing(inputPath, outputPath, per=False):\n",
        "  f = open(inputPath,'r')\n",
        "  fileText = f.readlines()\n",
        "  if per:\n",
        "    fileOut = [perNormalizer(text) + '\\n' for text in fileText]\n",
        "  else:\n",
        "    fileOut = [spell(text) for text in fileText]\n",
        "  with open(outputPath,'w') as f:\n",
        "    f.writelines(fileOut)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2-KTMQWOCXAD"
      },
      "outputs": [],
      "source": [
        "#پیش پردازش فایل ها\n",
        "load_and_pre_processing('data/test.en', 'data/test.en2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2oktnNecFND8"
      },
      "outputs": [],
      "source": [
        "load_and_pre_processing('data/test.fa', 'data/test.fa2', per=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "b9hA2JNnFPSW"
      },
      "outputs": [],
      "source": [
        "load_and_pre_processing('data/train.en', 'data/train.en2')\n",
        "load_and_pre_processing('data/train.fa', 'data/train.fa2', per=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "14RsSlb6FVnT"
      },
      "outputs": [],
      "source": [
        "load_and_pre_processing('data/valid.en', 'data/valid.en2')\n",
        "load_and_pre_processing('data/valid.fa', 'data/valid.fa2', per=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZOK7rAzrUFV"
      },
      "outputs": [],
      "source": [
        "text_files = ['test.en', 'test.fa', 'train.en', 'train.fa', 'valid.en', 'valid.fa']\n",
        "for fname in text_files:\n",
        "  f = open('data/'+fname)\n",
        "  tlist = f.readlines()\n",
        "  tlist = [tokenizer.encode(x) for x in tlist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "QvU5Aa_SA7k4"
      },
      "outputs": [],
      "source": [
        "#آموزش دادن توکنایزر\n",
        "!python /content/OpenNMT-tf/third_party/learn_bpe.py -i data/train.en2 -o data/en.code -s 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "HmeAWNhpBDfw"
      },
      "outputs": [],
      "source": [
        "!python /content/OpenNMT-tf/third_party/learn_bpe.py -i data/train.fa2 -o data/fa.code -s 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NayMgza2Btc3"
      },
      "outputs": [],
      "source": [
        "#اعمال توکنایزر\n",
        "!python /content/OpenNMT-tf/third_party/apply_bpe.py -c data/en.code -i data/train.en2 -o data/train.en.bpe\n",
        "!python /content/OpenNMT-tf/third_party/apply_bpe.py -c data/en.code -i data/valid.en2 -o data/valid.en.bpe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0IvQlfkOGXIj"
      },
      "outputs": [],
      "source": [
        "!python /content/OpenNMT-tf/third_party/apply_bpe.py -c data/en.code -i data/test.en2 -o data/test.en.bpe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "nlSx_NzwCDK3"
      },
      "outputs": [],
      "source": [
        "!python /content/OpenNMT-tf/third_party/apply_bpe.py -c data/fa.code -i data/train.fa2 -o data/train.fa.bpe\n",
        "!python /content/OpenNMT-tf/third_party/apply_bpe.py -c data/fa.code -i data/valid.fa2 -o data/valid.fa.bpe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnJFnwFZCC_l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KJ9W9BWx3vcs"
      },
      "outputs": [],
      "source": [
        "#تابع برای لود دیتافریم و نمایش آن در خروجی\n",
        "def loadDF(englishPath,persianPath):\n",
        "  f = open(englishPath,'r')\n",
        "  eng = f.readlines()\n",
        "  f.close()\n",
        "  f = open(persianPath,'r')\n",
        "  per = f.readlines()\n",
        "  f.close()\n",
        "  print('eng: ',len(eng))\n",
        "  print('per: ',len(per))\n",
        "  return pd.DataFrame.from_dict({'eng':eng, 'per':per})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4DU8zxzPlut",
        "outputId": "ed0d7b11-9ef2-4e95-c1fc-387aee377001"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eng:  30000\n",
            "per:  30000\n",
            "eng:  30000\n",
            "per:  30000\n",
            "eng:  400\n",
            "per:  400\n"
          ]
        }
      ],
      "source": [
        "trainDf2 = loadDF('/content/data/train.en', '/content/data/train.fa')\n",
        "trainDf = loadDF('/content/data/train.en.bpe', '/content/data/train.fa.bpe')\n",
        "validDf = loadDF('/content/data/valid.en.bpe', '/content/data/valid.fa.bpe')\n",
        "# testDf = loadDF('/content/data/test.en.bpe', '/content/data/test.fa.bpe')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rMioY6IGJ74"
      },
      "outputs": [],
      "source": [
        "trainDf2.head(10).style.hide_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGxbsylJClxY"
      },
      "outputs": [],
      "source": [
        "trainDf.head(10).style.hide_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C05ij4wcCzpA"
      },
      "outputs": [],
      "source": [
        "trainDf.style.hide_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "j33KCTawWn0W"
      },
      "outputs": [],
      "source": [
        "#ذخیره فایل خواسته شده در مورد بعد از پیش پردازش\n",
        "trainDf.to_csv('trainDf.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qf-mnJapWnyO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brpZXTXcWnvx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ke8y7edFWntC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk_Mr365gNPA"
      },
      "source": [
        "# openNMT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVZPSKUV5gZs",
        "outputId": "7ffc7533-b42f-45ed-ba60-6bec5bc1d053"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/data.zip\n",
            "   creating: data/\n",
            "  inflating: data/test.en            \n",
            "  inflating: data/test.fa            \n",
            "  inflating: data/train.en           \n",
            "  inflating: data/train.fa           \n",
            "  inflating: data/valid.en           \n",
            "  inflating: data/valid.fa           \n"
          ]
        }
      ],
      "source": [
        "!cp /content/drive/MyDrive/NLP/CA05/data.zip /content/data.zip\n",
        "!unzip /content/data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Jh8Sp7WgRQN",
        "outputId": "c9e37b73-4dee-4b45-b8bb-114b19c12095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'OpenNMT-tf'...\n",
            "remote: Enumerating objects: 34424, done.\u001b[K\n",
            "remote: Counting objects: 100% (6084/6084), done.\u001b[K\n",
            "remote: Compressing objects: 100% (792/792), done.\u001b[K\n",
            "remote: Total 34424 (delta 5304), reused 5914 (delta 5230), pack-reused 28340\u001b[K\n",
            "Receiving objects: 100% (34424/34424), 23.44 MiB | 23.67 MiB/s, done.\n",
            "Resolving deltas: 100% (28966/28966), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone -b r1 https://github.com/OpenNMT/OpenNMT-tf.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6RHKbMXHgR0l",
        "outputId": "54c0d299-95a7-4639-abec-97f5425288fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting OpenNMT-tf[tensorflow_gpu]==1.*\n",
            "  Downloading OpenNMT_tf-1.25.3-py2.py3-none-any.whl (150 kB)\n",
            "\u001b[K     |████████████████████████████████| 150 kB 7.2 MB/s \n",
            "\u001b[?25hCollecting rouge==0.3.1\n",
            "  Downloading rouge-0.3.1-py3-none-any.whl (7.4 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-tf[tensorflow_gpu]==1.*) (6.0)\n",
            "Collecting pyonmttok<2,>=1.11.0\n",
            "  Downloading pyonmttok-1.31.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (16.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.6 MB 46.4 MB/s \n",
            "\u001b[?25hCollecting sacrebleu==1.*\n",
            "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting tensorflow-gpu<2,>=1.4.0\n",
            "  Downloading tensorflow_gpu-1.15.5-cp37-cp37m-manylinux2010_x86_64.whl (411.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 411.0 MB 17 kB/s \n",
            "\u001b[?25hCollecting portalocker==2.0.0\n",
            "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu<2,>=1.4.0->OpenNMT-tf[tensorflow_gpu]==1.*) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu<2,>=1.4.0->OpenNMT-tf[tensorflow_gpu]==1.*) (0.2.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 41.3 MB/s \n",
            "\u001b[?25hCollecting h5py<=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 35.7 MB/s \n",
            "\u001b[?25hCollecting numpy<1.19.0,>=1.16.0\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 488 kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu<2,>=1.4.0->OpenNMT-tf[tensorflow_gpu]==1.*) (1.46.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu<2,>=1.4.0->OpenNMT-tf[tensorflow_gpu]==1.*) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu<2,>=1.4.0->OpenNMT-tf[tensorflow_gpu]==1.*) (1.14.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu<2,>=1.4.0->OpenNMT-tf[tensorflow_gpu]==1.*) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu<2,>=1.4.0->OpenNMT-tf[tensorflow_gpu]==1.*) (3.17.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu<2,>=1.4.0->OpenNMT-tf[tensorflow_gpu]==1.*) (3.3.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 59.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu<2,>=1.4.0->OpenNMT-tf[tensorflow_gpu]==1.*) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu<2,>=1.4.0->OpenNMT-tf[tensorflow_gpu]==1.*) (1.0.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu<2,>=1.4.0->OpenNMT-tf[tensorflow_gpu]==1.*) (0.8.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu<2,>=1.4.0->OpenNMT-tf[tensorflow_gpu]==1.*) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu<2,>=1.4.0->OpenNMT-tf[tensorflow_gpu]==1.*) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu<2,>=1.4.0->OpenNMT-tf[tensorflow_gpu]==1.*) (3.3.7)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu<2,>=1.4.0->OpenNMT-tf[tensorflow_gpu]==1.*) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu<2,>=1.4.0->OpenNMT-tf[tensorflow_gpu]==1.*) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu<2,>=1.4.0->OpenNMT-tf[tensorflow_gpu]==1.*) (4.2.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=55324e1040185a676127c1fa4a89ca7e6dbafe51822703889ab9fd06725ca64c\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: numpy, portalocker, h5py, tensorflow-estimator, tensorboard, sacrebleu, rouge, pyonmttok, keras-applications, gast, tensorflow-gpu, OpenNMT-tf\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220527125636 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220527125636 requires tensorboard<2.9,>=2.8, but you have tensorboard 1.15.0 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220527125636 requires tensorflow-estimator<2.9,>=2.8, but you have tensorflow-estimator 1.15.1 which is incompatible.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "jaxlib 0.3.7+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "jax 0.3.8 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed OpenNMT-tf-1.25.3 gast-0.2.2 h5py-2.10.0 keras-applications-1.0.8 numpy-1.18.5 portalocker-2.0.0 pyonmttok-1.31.0 rouge-0.3.1 sacrebleu-1.5.1 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.5\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "h5py",
                  "numpy",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install OpenNMT-tf[tensorflow_gpu]==1.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d32krp4ihGvI"
      },
      "outputs": [],
      "source": [
        "!python OpenNMT-tf/third_party/learn_bpe.py -i data/train.en -o data/en.code -s 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j57-_y_vhGvJ"
      },
      "outputs": [],
      "source": [
        "!python OpenNMT-tf/third_party/learn_bpe.py -i data/train.fa -o data/fa.code -s 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xata7oL7gRvn"
      },
      "outputs": [],
      "source": [
        "!python OpenNMT-tf/third_party/apply_bpe.py -c  data/en.code -i  /content/data/train.en -o /content/data/train.en.bpe\n",
        "!python OpenNMT-tf/third_party/apply_bpe.py -c  data/en.code -i  /content/data/valid.en -o /content/data/valid.en.bpe\n",
        "!python OpenNMT-tf/third_party/apply_bpe.py -c data/en.code -i /content/data/test.en -o /content/data/test.en.bpe\n",
        "!python OpenNMT-tf/third_party/apply_bpe.py -c data/fa.code -i /content/data/train.fa -o /content/data/train.fa.bpe\n",
        "!python OpenNMT-tf/third_party/apply_bpe.py -c data/fa.code -i /content/data/valid.fa -o /content/data/valid.fa.bpe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5Igdl8ngRsz",
        "outputId": "d8bed9e1-d7a1-43f9-f68a-74ab42662795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/decoders/rnn_decoder.py:435: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/optimizers/adafactor.py:32: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/optimizers/multistep_adam.py:36: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!onmt-build-vocab --size 50000 --save_vocab /content/data/en-vocab.txt /content/data/train.en.bpe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xTirNjagRqr",
        "outputId": "be3faa44-9fd3-4e36-c5b0-bbdeeb65d893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/decoders/rnn_decoder.py:435: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/optimizers/adafactor.py:32: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/optimizers/multistep_adam.py:36: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!onmt-build-vocab --size 50000 --save_vocab /content/data/fa-vocab.txt /content/data/train.fa.bpe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFlV3jqjsL0H"
      },
      "outputs": [],
      "source": [
        "#کپی کردن فایل کانفیگ در مسیر پردازش\n",
        "!cp /content/drive/MyDrive/NLP/CA05/data.yml /content/data.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0At1jYbJjbmw",
        "outputId": "362f6f5d-bcdd-41e8-f621-20f70529604f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ema4itQgRn2",
        "outputId": "65aec2f7-4065-476b-992a-4253f5033260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/decoders/rnn_decoder.py:435: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/optimizers/adafactor.py:32: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/optimizers/multistep_adam.py:36: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/bin/main.py:111: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/bin/main.py:111: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/bin/main.py:145: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/config.py:95: The name tf.logging.warn is deprecated. Please use tf.compat.v1.logging.warn instead.\n",
            "\n",
            "WARNING:tensorflow:You provided a model configuration but a checkpoint already exists. The model configuration must define the same model as the one used for the initial training. However, you can change non structural values like dropout.\n",
            "2022-06-05 14:56:06.596873: W tensorflow/python/util/util.cc:299] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/bin/main.py:154: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/bin/main.py:157: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/runner.py:97: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:Using parameters:\n",
            "data:\n",
            "  eval_features_file: /content/data/valid.en.bpe\n",
            "  eval_labels_file: /content/data/valid.fa.bpe\n",
            "  source_words_vocabulary: /content/data/en-vocab.txt\n",
            "  target_words_vocabulary: /content/data/fa-vocab.txt\n",
            "  train_features_file: /content/data/train.en.bpe\n",
            "  train_labels_file: /content/data/train.fa.bpe\n",
            "eval:\n",
            "  batch_size: 30\n",
            "  eval_delay: 1800\n",
            "  export_format: saved_model\n",
            "  export_on_best: bleu\n",
            "  exporters: last\n",
            "  length_bucket_width: 5\n",
            "  max_exports_to_keep: 15\n",
            "  save_eval_predictions: true\n",
            "  scorers: bleu, rouge, wer, ter, prf\n",
            "  steps: 1000\n",
            "infer:\n",
            "  batch_size: 128\n",
            "  bucket_width: null\n",
            "model_dir: /content/drive/MyDrive/NLP/CA05/run/\n",
            "params:\n",
            "  beam_width: 5\n",
            "  clip_gradients: 5.0\n",
            "  decay_params:\n",
            "    decay_rate: 0.7\n",
            "    decay_steps: 100000\n",
            "  decay_type: exponential_decay\n",
            "  learning_rate: 0.5\n",
            "  maximum_iterations: 50\n",
            "  optimizer: GradientDescentOptimizer\n",
            "  param_init: 0.1\n",
            "  start_decay_steps: 500000\n",
            "score:\n",
            "  batch_size: 64\n",
            "train:\n",
            "  batch_size: 128\n",
            "  batch_type: examples\n",
            "  bucket_width: 1\n",
            "  maximum_features_length: 512\n",
            "  maximum_labels_length: 512\n",
            "  sample_buffer_size: -1\n",
            "  save_checkpoints_steps: 2500\n",
            "  save_summary_steps: 50\n",
            "  train_steps: 50000\n",
            "\n",
            "2022-06-05 14:56:06.648538: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-06-05 14:56:06.653580: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2022-06-05 14:56:06.653803: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x30a59c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-06-05 14:56:06.653838: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2022-06-05 14:56:06.655866: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2022-06-05 14:56:06.665428: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2022-06-05 14:56:06.665468: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (b7f07643a586): /proc/driver/nvidia/version does not exist\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/content/drive/MyDrive/NLP/CA05/run/', '_tf_random_seed': None, '_save_summary_steps': 50, '_save_checkpoints_steps': 2500, '_save_checkpoints_secs': None, '_session_config': gpu_options {\n",
            "}\n",
            "allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    layout_optimizer: OFF\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f80b7a0bc90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 2500 or save_checkpoints_secs None.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "INFO:tensorflow:Training on 30000 examples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/tokenizers/tokenizer.py:284: calling string_split (from tensorflow.python.ops.ragged.ragged_string_ops) with delimiter is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "delimiter is deprecated, please use sep instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.data.get_output_shapes is deprecated. Please use tf.compat.v1.data.get_output_shapes instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/estimator.py:128: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/estimator.py:130: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/estimator.py:130: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/utils/parallel.py:143: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/utils/parallel.py:144: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/models/transformer.py:136: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/encoders/self_attention_encoder.py:59: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/core.py:271: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/layers/transformer.py:136: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv1D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/models/sequence_to_sequence.py:201: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/decoders/decoder.py:63: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/estimator.py:261: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/utils/optim.py:80: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/utils/optim.py:111: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/utils/optim.py:330: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/utils/optim.py:322: The name tf.global_norm is deprecated. Please use tf.linalg.global_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/inputters/text_inputter.py:44: The name tf.gfile.Copy is deprecated. Please use tf.io.gfile.copy instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/inputters/text_inputter.py:47: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/inputters/text_inputter.py:75: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/utils/hooks.py:132: The name tf.train.SecondOrStepTimer is deprecated. Please use tf.estimator.SecondOrStepTimer instead.\n",
            "\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Number of trainable parameters: 59490605\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/utils/hooks.py:152: The name tf.summary.FileWriterCache is deprecated. Please use tf.compat.v1.summary.FileWriterCache instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/utils/hooks.py:154: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/utils/hooks.py:157: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/utils/hooks.py:159: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.\n",
            "\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/NLP/CA05/run/model.ckpt-17500\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 17500 into /content/drive/MyDrive/NLP/CA05/run/model.ckpt.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/opennmt/utils/hooks.py:171: The name tf.train.SessionRunArgs is deprecated. Please use tf.estimator.SessionRunArgs instead.\n",
            "\n",
            "INFO:tensorflow:loss = 1.8120438, step = 17501\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "#شروع آموزش\n",
        "!onmt-main train_and_eval --model_type Transformer --config data.yml --num_gpus 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#تا گام 17500 در این داکیومنت موجود است و ادامه فرآیند در فایل بعدی می‌باشد\n",
        "#به دلیل طولانی شدن مدت پردازش و ارائه نکردن پردازشگر گرافیکی توسط گوگل در کولب"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c76gRSs9jRkb"
      },
      "outputs": [],
      "source": [
        "# !cp -R /content/drive/MyDrive/NLP/CA05/run /content/drive/MyDrive/NLP/run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvHDYbL_jRiY",
        "outputId": "f5a40d22-3032-4a06-8cb9-26764ea10662"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.9G\t/content/drive/MyDrive/NLP/CA05/run\n"
          ]
        }
      ],
      "source": [
        "!du -sh /content/drive/MyDrive/NLP/CA05/run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdzgaYSJjRgZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2SubRujjReI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHJHtf-fjRbX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "H9fgrSkh4VEq",
        "QP38PV2gz7xm"
      ],
      "name": "NLP_CA05.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
