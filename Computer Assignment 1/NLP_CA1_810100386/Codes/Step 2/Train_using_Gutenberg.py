#Hosein Seifi ÿ≠ÿ≥€åŸÜ ÿ≥€åŸÅ€å
#810100386
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.models import WordPiece
from tokenizers.trainers import WordPieceTrainer
from tokenizers.pre_tokenizers import Whitespace
gutenberg = ["pg16457.txt"]
#%%
BPEtokenizer = Tokenizer(BPE(unk_token="[UNK]"))
bpe_trainer = BpeTrainer(vocab_size = 1000000, special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])
BPEtokenizer.pre_tokenizer = Whitespace()
BPEtokenizer.train(gutenberg, bpe_trainer)
vocab_bpe = BPEtokenizer.get_vocab()
#%%
output_bpe = BPEtokenizer.encode("This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!üòÅ").tokens
#%%
WPtokenizer = Tokenizer(WordPiece(unk_token="[UNK]"))
wp_trainer = WordPieceTrainer(vocab_size = 1000000, special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])
WPtokenizer.pre_tokenizer = Whitespace()
WPtokenizer.train(gutenberg, wp_trainer)
vocab_wp = WPtokenizer.get_vocab()
#%%
output_wp = WPtokenizer.encode("This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!üòÅ").tokens
#%%